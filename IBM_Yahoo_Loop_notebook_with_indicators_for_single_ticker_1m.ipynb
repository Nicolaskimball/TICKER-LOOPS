{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IBM Yahoo Loop notebook with indicators for single ticker 1m.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicolaskimball/TICKER-LOOPS/blob/main/IBM_Yahoo_Loop_notebook_with_indicators_for_single_ticker_1m.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULgLIJx4zqGB",
        "outputId": "83762767-41db-4862-bb77-1d46bd522c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!pip install -q alpha_vantage mplfinance yfinance tensorflow_addons finta schedule"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 61kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.5MB 46.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 52.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 65.0MB/s \n",
            "\u001b[?25h  Building wheel for alpha-vantage (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvfHqtwFmh8X",
        "outputId": "ecdde154-f955-4506-e308-0584019d77c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "# download TA-Lib \n",
        "!wget -q http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz 2>&1 >/dev/null\n",
        "!tar xzf ta-lib-0.4.0-src.tar.gz 2>&1 >/dev/null\n",
        "\n",
        "import os\n",
        "\n",
        "os.chdir('ta-lib') # Can't use !cd in co-lab\n",
        "\n",
        "!./configure --prefix=/usr --enable-silent-rules 2>&1 >/dev/null\n",
        "!make -s 2>&1 >/dev/null\n",
        "!make install 2>&1 >/dev/null\n",
        "\n",
        "os.chdir('../')\n",
        "\n",
        "!pip install -q TA-Lib 2>&1 > /dev/null\n",
        "\n",
        "import talib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./configure: line 4354: /usr/bin/file: No such file or directory\n",
            "libtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "\u001b[01m\u001b[Kgen_code.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KprintFuncHeaderDoc\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kgen_code.c:3456:4:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat not a string literal and no format arguments [\u001b[01;35m\u001b[K-Wformat-security\u001b[m\u001b[K]\n",
            "    \u001b[01;35m\u001b[Kfprintf\u001b[m\u001b[K( out, prefix );\n",
            "    \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svmb8E7bzZvb"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as kb\n",
        "from sklearn import preprocessing\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
        "from collections import deque\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import yfinance as yf\n",
        "import finta\n",
        "import schedule\n",
        "\n",
        "\n",
        "API_KEY='H_ED__vbG_DJyjPiYKboH_Vf_G4Oq_dTkDhkPi'\n",
        "API_KEY='I2C7Y6B8KB5MCA6N'\n",
        "## UPDATE TICKER HERE.. \n",
        "TICKER = \"spy\"\n",
        "\n",
        "# Window size or the sequence length\n",
        "N_STEPS = 20\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 5\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "### model parameters\n",
        "N_LAYERS = 5\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 512\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "### training parameters\n",
        "# mean squared error loss\n",
        "LOSS = \"msle\"\n",
        "OPTIMIZER = tfa.optimizers.RectifiedAdam(0.001)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 400\n",
        "PATIENCE = 10\n",
        "\n",
        "# Data loading parameters\n",
        "INTERVAL = \"5m\"\n",
        "PERIOD   = \"5000m\"\n",
        "\n",
        "## DONT TOUCH ANYTHING BELOW HERE...\n",
        "ticker = TICKER\n",
        "FEATURE_COLUMNS = [\n",
        "    \"open\",\n",
        "    \"close\",\n",
        "    \"volume\",\n",
        "    \"high\",\n",
        "    \"low\",\n",
        "    \"RSI\",\n",
        "    \"MacD\",\n",
        "    \"OBV\",\n",
        "    \"HMA\",\n",
        "    \"EMA\",\n",
        "]\n",
        "\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "def load_data(ticker,\n",
        "              n_steps=50,\n",
        "              scale=True,\n",
        "              shuffle=True,\n",
        "              lookup_step=1, \n",
        "              test_size=0.2, \n",
        "              feature_columns=[\n",
        "                  \"close\",\n",
        "                  \"high\",\n",
        "                  \"low\",\n",
        "                  \"volume\",\n",
        "                  \"low\",\n",
        "                  \"RSI\",\n",
        "                  \"MacD\",\n",
        "                  \"OBV\",\n",
        "                  \"HMA\",\n",
        "                  \"EMA\",\n",
        "\n",
        "]\n",
        "):\n",
        "\n",
        "    amd = yf.Ticker(ticker)\n",
        "    df = amd.history(period=PERIOD, interval=INTERVAL)\n",
        "\n",
        "    # Print the properties of the input data\n",
        "    print(df.columns)\n",
        "    print(f'Number of input records: {df.shape[0]:,}')\n",
        "    print(f'Length of input records: {df.shape[1]:,}')\n",
        "\n",
        "    # Dictionary 'dict' or map or hash map\n",
        "    d = {'from': 'to'}\n",
        "\n",
        "    # The data is in reverse order - put it so time moves to the right/down\n",
        "    # df = df.sort_index(axis=0, ascending=True)\n",
        "    # Assign new column names\n",
        "    df.rename(\n",
        "      columns={\n",
        "         'Open'  : 'open',\n",
        "         'Close' : 'close',\n",
        "         'High'  : 'high',\n",
        "         'Low'   : 'low',\n",
        "         'Volume': 'volume',\n",
        "      },\n",
        "      inplace=True\n",
        "    )\n",
        "    print(df)\n",
        "\n",
        "    # - Relative Strength Indicator\n",
        "    real = talib.RSI(df['close'], timeperiod=14)\n",
        "    df['RSI'] = real\n",
        "\n",
        "    # Momentum Signal - Moving Average Convergence Divergence\n",
        "    macd, macdsignal, macdhist = talib.MACD(\n",
        "        df['close'],\n",
        "        fastperiod=12,\n",
        "        slowperiod=26,\n",
        "        signalperiod=9\n",
        "    )\n",
        "    df['MacD'] = macd\n",
        "\n",
        "    # Volume Signal - On Balance Volume\n",
        "    obv = talib.OBV(\n",
        "        df['close'],\n",
        "        df['volume']\n",
        "    )\n",
        "    df['OBV'] = obv\n",
        "\n",
        "    # Hull Moving Average\n",
        "    hma = finta.TA.HMA(df)\n",
        "    df['HMA'] = hma\n",
        "\n",
        "    ema = talib.EMA(df['close'])\n",
        "    df['EMA'] = ema\n",
        "\n",
        "    # Trim the DataFrame to just those values without the initial nulls from the window functions\n",
        "    first_all_cols_non_null_index = df.apply(lambda series: series.first_valid_index()).max()\n",
        "    df = df[df.index > first_all_cols_non_null_index]\n",
        "\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['close'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
        "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
        "    last_sequence = list(sequences) + list(last_sequence)\n",
        "    # shift the last sequence by -1\n",
        "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # reshape X to fit the neural network\n",
        "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
        "    # split the dataset\n",
        "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
        "    # return the result\n",
        "    return result\n",
        "\n",
        "\n",
        "def create_model(input_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\"):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            model.add(cell(units, return_sequences=True, input_shape=(None, input_length)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_accuracy(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"close\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"close\"].inverse_transform(y_pred))\n",
        "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
        "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
        "    return {\n",
        "        'accuracy_percentage': accuracy_score(y_test, y_pred) * 100, \n",
        "        'mean_absolute_error': mean_absolute_error(y_test, y_pred),\n",
        "        'mean_squared_error': mean_squared_error(y_test, y_pred),\n",
        "        'mean_squared_log_error': mean_squared_log_error(y_test, y_pred),\n",
        "        'r_squared_score': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "def predict(model, data, classification=False):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
        "    # retrieve the column scalers\n",
        "    column_scaler = data[\"column_scaler\"]\n",
        "    # reshape the last sequence\n",
        "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    predicted_price = column_scaler[\"close\"].inverse_transform(prediction)[0][0]\n",
        "    return predicted_price\n",
        "\n",
        "\n",
        "def plot_graph(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"close\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"close\"].inverse_transform(y_pred))\n",
        "    # last 200 days, feel free to edit that\n",
        "    plt.plot(y_test[-12:], c='b')\n",
        "    plt.plot(y_pred[-12:], c='r')\n",
        "    plt.xlabel(INTERVAL)\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def r2_score_keras(y_true, y_pred):\n",
        "    \"\"\"Implements the Coeffecient of Determination, R^2 or R-squared\"\"\"\n",
        "    SS_res =  kb.sum(kb.square(y_true - y_pred))\n",
        "    SS_tot = kb.sum(kb.square(y_true - kb.mean(y_true)))\n",
        "    return (1 - SS_res / (SS_tot + kb.epsilon()))\n",
        "\n",
        "\n",
        "def inverse_r2_score_keras(y_true, y_pred):\n",
        "    \"\"\"Implements the inverse Coeffecient of Determination, R^2 or R-squared\"\"\"\n",
        "    SS_res =  kb.sum(kb.square(y_true - y_pred))\n",
        "    SS_tot = kb.sum(kb.square(y_true - kb.mean(y_true)))\n",
        "    return 1.000000000 / (1 - SS_res / (SS_tot + kb.epsilon()))\n",
        "\n",
        "\n",
        "def new_predict(ticker, model, scalers):\n",
        "    \"\"\"Given a ticker and model, make predictions for a yahoo Ticker\"\"\"\n",
        "    yahoo_ticker = yf.Ticker(ticker)\n",
        "\n",
        "    yahoo_df = yahoo_ticker.history(period=PERIOD, interval=INTERVAL)\n",
        "    yahoo_df['DateTimeBarStart'] = yahoo_df.index\n",
        "    yahoo_df['DateTimeBarStart'] = yahoo_df['DateTimeBarStart'].dt.tz_localize(None)\n",
        "    yahoo_df = yahoo_df.set_index('DateTimeBarStart')\n",
        "\n",
        "    yahoo_df = yahoo_df.rename(\n",
        "        columns={\n",
        "            'Close': 'close',\n",
        "            'Volume': 'volume',\n",
        "        },\n",
        "    )\n",
        "\n",
        "    yahoo_df.rename(\n",
        "      columns={\n",
        "         'Open'  : 'open',\n",
        "         'Close' : 'close',\n",
        "         'High'  : 'high',\n",
        "         'Low'   : 'low',\n",
        "         'Volume': 'volume',\n",
        "      },\n",
        "      inplace=True\n",
        "    )\n",
        "\n",
        "    # - Relative Strength Indicator\n",
        "    real = talib.RSI(yahoo_df['close'], timeperiod=14)\n",
        "    yahoo_df['RSI'] = real\n",
        "\n",
        "    # Momentum Signal - Moving Average Convergence Divergence\n",
        "    macd, macdsignal, macdhist = talib.MACD(\n",
        "        yahoo_df['close'],\n",
        "        fastperiod=12,\n",
        "        slowperiod=26,\n",
        "        signalperiod=9\n",
        "    )\n",
        "    yahoo_df['MacD'] = macd\n",
        "\n",
        "    # Volume Signal - On Balance Volume\n",
        "    obv = talib.OBV(\n",
        "        yahoo_df['close'],\n",
        "        yahoo_df['volume']\n",
        "    )\n",
        "    yahoo_df['OBV'] = obv\n",
        "\n",
        "    # Hull Moving Average\n",
        "    hma = finta.TA.HMA(yahoo_df)\n",
        "    yahoo_df['HMA'] = hma\n",
        "\n",
        "    ema = talib.EMA(yahoo_df['close'])\n",
        "    yahoo_df['EMA'] = ema\n",
        "\n",
        "    # Retain only feautre columns\n",
        "    yahoo_df = yahoo_df[FEATURE_COLUMNS]\n",
        "\n",
        "    # During the trading day, drop the last, incomplete minute\n",
        "    yahoo_df = yahoo_df.drop(\n",
        "        yahoo_df.tail(1).index\n",
        "    )\n",
        "\n",
        "    # Must transform/scale the data from Yahoo using our Min/Max scaler for training data\n",
        "    for column in FEATURE_COLUMNS:\n",
        "        yahoo_df[f'scaled_{column}'] = scalers[column].transform(\n",
        "            np.expand_dims(yahoo_df[column], 1)\n",
        "        )\n",
        "\n",
        "    scaled_column_names = [f'scaled_{x}' for x in FEATURE_COLUMNS]\n",
        "    scaled_rename_dict  = {f'scaled_{x}': x for x in FEATURE_COLUMNS}\n",
        "    scaled_yahoo_df = yahoo_df[scaled_column_names].rename(\n",
        "        scaled_rename_dict\n",
        "    )\n",
        "\n",
        "    predict_df = scaled_yahoo_df.iloc[-N_STEPS:]\n",
        "\n",
        "    # Expand the dimensions so we have an array of one time series window\n",
        "    predict_ary = np.expand_dims(\n",
        "        predict_df.values,\n",
        "        axis=0\n",
        "    )\n",
        "\n",
        "    columns = []\n",
        "    for i in range(predict_ary.shape[2]):\n",
        "        columns.append(\n",
        "            predict_ary[0, :, i]\n",
        "        )\n",
        "    column_predict_ary = np.expand_dims(np.stack(columns), axis=0)\n",
        "\n",
        "    raw_prediction = model.predict(column_predict_ary)\n",
        "    prediction = scalers['close'].inverse_transform(\n",
        "        raw_prediction\n",
        "    )[0][0]\n",
        "\n",
        "    # Get the last timestamp, add the LOOKUP_STEP * 1 minute to get the predicted time\n",
        "    last_dt = yahoo_df.index[-1]\n",
        "    last_price = yahoo_df.iloc[-1]['close']\n",
        "\n",
        "    return prediction, last_dt, last_price\n",
        "\n",
        "\n",
        "def print_row(first=False, scalers=None):\n",
        "    prediction, last_dt, last_price = new_predict(TICKER, model, scalers)\n",
        "\n",
        "    last_iso_dt = last_dt.isoformat().replace('T', ' ')\n",
        "\n",
        "    headers = ['DateTime', 'Price', 'Predicted DateTime', 'Predicted Price']\n",
        "    if first:\n",
        "        print(\n",
        "            '{:<20s}    {:<10s}    {:<20s}    {:<20s}'.format(*headers)\n",
        "        )\n",
        "\n",
        "    future_iso_dt = (last_dt + timedelta(minutes=LOOKUP_STEP)).isoformat().replace('T', ' ')\n",
        "\n",
        "    formatted_last_price = '{:<.4f}'.format(last_price)\n",
        "    formatted_prediction = '{:<.4f}'.format(prediction)\n",
        "\n",
        "    print(\n",
        "        '{:<20s}    {:<10s}    {:<20s}    {:<20s}'.format(\n",
        "            last_iso_dt, formatted_last_price, future_iso_dt, formatted_prediction\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Disable printing\n",
        "def block_print():\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "\n",
        "# Restore printing\n",
        "def enable_print():\n",
        "    sys.stdout = sys.__stdout__\n",
        "\n",
        "\n",
        "# model name to save\n",
        "model_name = f\"{date_now}_{ticker}-{LOSS}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "\n",
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzhkkT0ZT-Xn"
      },
      "source": [
        "# Load or update the data the model trains on\n",
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_ISgjPyQFs-"
      },
      "source": [
        "# Remove the previous model\n",
        "try:\n",
        "    shutil.rmtree(os.path.join('results', 'temp_model'))\n",
        "except FileNotFoundError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gpBAhQUu2n"
      },
      "source": [
        "# Train a new model\n",
        "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                     dropout=DROPOUT, optimizer=OPTIMIZER)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name), save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, verbose=1)\n",
        "\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard, early_stopping],\n",
        "                    verbose=1)\n",
        "\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlX6N1gWTHKX"
      },
      "source": [
        "# Load the model we already trained and evaluate it\n",
        "model.load_weights(model_path)\n",
        "print(len(data[\"df\"].index))\n",
        "pd.DataFrame([get_accuracy(model, data)])      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v3ASz4ePp02"
      },
      "source": [
        "\n",
        "# Make a prediction using the model we previously trained\n",
        "model.load_weights(model_path)\n",
        "\n",
        "# Load the latest Yahoo data\n",
        "del data\n",
        "data = load_data(\n",
        "    ticker,\n",
        "    N_STEPS,\n",
        "    lookup_step=LOOKUP_STEP,\n",
        "    test_size=TEST_SIZE,\n",
        "    feature_columns=FEATURE_COLUMNS,\n",
        ")\n",
        "# print(f'Last sequence: {data[\"last_sequence\"]}')\n",
        "print(f'Data updated. Last record is: {data[\"df\"].index[-1]}')\n",
        "\n",
        "# predict the future price\n",
        "future_price = predict(model, data)\n",
        "print(f\"Future price after {LOOKUP_STEP} {INTERVAL} is {future_price:.2f}$\")\n",
        "\n",
        "# Plot the graph\n",
        "# plot_graph(model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5tL8Q0mi95o"
      },
      "source": [
        "# Print the first row with header\n",
        "schedule.clear()\n",
        "schedule.every(5).minutes.at(\":05\").do(print_row, first=False, scalers=data['column_scaler'])\n",
        "# schedule.every(5).seconds.do(print_row, first=False, scalers=data['column_scaler'])\n",
        "\n",
        "print_row(first=True, scalers=data['column_scaler'])\n",
        "# Check the schedule ever second and run any scheduled events\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZlraEr8Yu3u"
      },
      "source": [
        "first=True\n",
        "prediction, last_dt, last_price = new_predict(TICKER, model, data['column_scaler'])\n",
        "prediction, last_dt, last_price\n",
        "\n",
        "last_iso_dt = last_dt.isoformat().replace('T', ' ')\n",
        "\n",
        "headers = ['DateTime', 'Price', 'Predicted DateTime', 'Predicted Price']\n",
        "if first:\n",
        "    print(\n",
        "        '{:<20s}    {:<10s}    {:<20s}    {:<20s}'.format(*headers)\n",
        "    )\n",
        "\n",
        "future_iso_dt = (last_dt + timedelta(minutes=LOOKUP_STEP)).isoformat().replace('T', ' ')\n",
        "\n",
        "formatted_last_price = '{:<.4f}'.format(last_price)\n",
        "formatted_prediction = '{:<.4f}'.format(prediction)\n",
        "\n",
        "a = \\\n",
        "    '{:<20s}    {:<10s}    {:<20s}    {:<20s}'.format(\n",
        "        last_iso_dt, formatted_last_price, future_iso_dt, formatted_prediction\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWM6ICLEcoml"
      },
      "source": [
        "print(last_iso_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLZbcPD3i6dA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}